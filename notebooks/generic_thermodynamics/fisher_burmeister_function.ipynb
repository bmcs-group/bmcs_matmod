{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, you can incorporate the Kuhn-Tucker conditions symbolically into your SymPy computations to handle both active and inactive constraints automatically within the Newton-Raphson method, without resorting to explicit `if-else` conditions in your numerical implementation. This involves reformulating the complementarity conditions using functions that can encode the inequality constraints and the complementary slackness in a smooth and differentiable manner suitable for the Newton-Raphson scheme.\n",
    "\n",
    "### Reformulating Complementarity Conditions\n",
    "\n",
    "The Kuhn-Tucker conditions for inequality constraints involve:\n",
    "\n",
    "1. **Stationarity**:‚ÄÉ‚àáf(ùúñ) + Œª‚àág(ùúñ) = 0\n",
    "2. **Primal Feasibility**:‚ÄÉg(ùúñ) ‚â§ 0\n",
    "3. **Dual Feasibility**:‚ÄÉŒª ‚â• 0\n",
    "4. **Complementary Slackness**:‚ÄÉŒªg(ùúñ) = 0\n",
    "\n",
    "Instead of handling the inequalities and the complementarity condition separately, you can encapsulate them into a single equation using a nonlinear complementarity (NCP) function. One such function is the **Fischer-Burmeister (FB) function**:\n",
    "\n",
    "$$\n",
    "\\phi_{\\text{FB}}(a, b) = \\sqrt{a^2 + b^2} - (a + b)\n",
    "$$\n",
    "\n",
    "The complementarity condition $ a \\geq 0 $, $ b \\geq 0 $, $ ab = 0 $ is equivalent to $ \\phi_{\\text{FB}}(a, b) = 0 $. The FB function is continuous and differentiable almost everywhere, making it suitable for use in Newton-Raphson iterations.\n",
    "\n",
    "### Incorporating the FB Function into Your Residual\n",
    "\n",
    "You can reformulate the complementarity condition $ \\lambda g(ùúñ) = 0 $ using the FB function:\n",
    "\n",
    "$$\n",
    "\\phi_{\\text{FB}}(\\lambda, -g(ùúñ)) = 0\n",
    "$$\n",
    "\n",
    "This equation replaces the need to separately enforce $ \\lambda \\geq 0 $, $ g(ùúñ) \\leq 0 $, and $ \\lambda g(ùúñ) = 0 $, and can be included directly in your residual vector $ R $. Your system of equations for the Newton-Raphson method becomes:\n",
    "\n",
    "$$\n",
    "R(\\epsilon, \\lambda) = \n",
    "\\begin{cases}\n",
    "\\displaystyle \\frac{\\partial L}{\\partial \\epsilon} = 0 \\\\\n",
    "\\phi_{\\text{FB}}(\\lambda, -g(\\epsilon)) = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "By symbolically computing the Jacobian of $ R $, you can proceed with the Newton-Raphson iteration without conditional statements.\n",
    "\n",
    "### Implementing in SymPy\n",
    "\n",
    "Here's how you might implement this in SymPy:\n",
    "\n",
    "```python\n",
    "import sympy as sp\n",
    "\n",
    "# Define symbols\n",
    "epsilon, lambda_ = sp.symbols('epsilon lambda', real=True, nonnegative=True)\n",
    "A = sp.Matrix([epsilon, lambda_])\n",
    "\n",
    "# Define your functions\n",
    "f = ...  # Define f(epsilon)\n",
    "g = ...  # Define g(epsilon)\n",
    "\n",
    "# Fischer-Burmeister function\n",
    "def phi_FB(a, b):\n",
    "    return sp.sqrt(a**2 + b**2) - (a + b)\n",
    "\n",
    "# Lagrangian\n",
    "L = f + lambda_ * g\n",
    "\n",
    "# Residuals\n",
    "R1 = sp.diff(L, epsilon)\n",
    "R2 = phi_FB(lambda_, -g)\n",
    "\n",
    "R = sp.Matrix([R1, R2])\n",
    "\n",
    "# Jacobian\n",
    "J = R.jacobian(A)\n",
    "\n",
    "# Lambdify for numerical computation\n",
    "R_func = sp.lambdify((epsilon, lambda_), R, modules='numpy')\n",
    "J_func = sp.lambdify((epsilon, lambda_), J, modules='numpy')\n",
    "```\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- **Automatic Handling**: The complementarity condition is handled implicitly, and the Newton-Raphson iterations adjust $ \\lambda $ and $ ùúñ $ accordingly without explicit checks.\n",
    "- **Symbolic Differentiation**: By expressing everything symbolically, you can leverage SymPy's capabilities to compute derivatives accurately.\n",
    "- **Smoothness**: Functions like the FB function are designed to be smooth enough for Newton-Raphson methods.\n",
    "\n",
    "### Considerations\n",
    "\n",
    "- **Differentiability**: Ensure that the NCP function you choose is sufficiently differentiable for the Newton-Raphson method to converge.\n",
    "- **Initial Guesses**: Good initial guesses may be necessary, especially since NCP functions can introduce nonlinearity.\n",
    "- **Convergence**: Monitor convergence carefully. Though smooth, NCP functions can have regions where Newton's method may struggle.\n",
    "\n",
    "### Alternative NCP Functions\n",
    "\n",
    "Other NCP functions you might consider include:\n",
    "\n",
    "- **Chen-Harker-Kanzow-Smale function**:\n",
    "\n",
    "  $$\n",
    "  \\phi_{\\text{CHKS}}(a, b) = a + b - \\sqrt{a^2 + b^2}\n",
    "  $$\n",
    "\n",
    "- **Smooth Approximation**:\n",
    "\n",
    "  You can approximate the complementarity condition using a penalty function or a smoothed minimum function.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "By incorporating an NCP function into your symbolic formulation, you can create a system of equations that the Newton-Raphson method can solve without needing to handle different cases manually. This approach aligns well with mathematical optimization practices and leverages the power of symbolic computation to produce efficient and robust numerical algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "sp.init_printing()\n",
    "\n",
    "# Define symbols\n",
    "epsilon, lambda_ = sp.symbols('epsilon lambda', real=True, nonnegative=True)\n",
    "A = sp.Matrix([epsilon, lambda_])\n",
    "\n",
    "# Define your functions\n",
    "f = sp.Function('f')(epsilon)  # Define f(epsilon)\n",
    "g = sp.Function('g')(epsilon)  # Define g(epsilon)\n",
    "\n",
    "# Fischer-Burmeister function\n",
    "def phi_FB(a, b):\n",
    "    return sp.sqrt(a**2 + b**2) - (a + b)\n",
    "\n",
    "# Lagrangian\n",
    "L = f + lambda_ * g\n",
    "\n",
    "# Residuals\n",
    "R1 = sp.diff(L, epsilon)\n",
    "R2 = phi_FB(lambda_, -g)\n",
    "\n",
    "R = sp.Matrix([R1, R2])\n",
    "\n",
    "# Jacobian\n",
    "J = R.jacobian(A)\n",
    "\n",
    "R, J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Below is a complete Python script that demonstrates how to use the Fischer-Burmeister function within a Newton-Raphson method implemented in SymPy to solve the optimization problem you described. This script includes both cases:\n",
    "\n",
    "1. **Case 1**: The unconstrained minimum is within the admissible range.\n",
    "2. **Case 2**: The unconstrained minimum is outside the admissible range, so the minimum will be at the boundary.\n",
    "\n",
    "The optimization problem is set up with:\n",
    "\n",
    "- A quadratic objective function $ f(\\epsilon) $ with its minimum at $ \\epsilon_{\\text{min}} $.\n",
    "- An inequality constraint $ g(\\epsilon) \\leq 0 $ defining the admissible range with midpoint $ \\epsilon_{\\text{mid}} $ and radius $ \\epsilon_{\\text{rad}} $.\n",
    "\n",
    "The script symbolically defines the problem, computes the residuals and Jacobian, and then solves it numerically using the Newton-Raphson method.\n",
    "\n",
    "```python\n",
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "# Define symbols\n",
    "epsilon, lambda_ = sp.symbols('epsilon lambda', real=True, nonnegative=True)\n",
    "\n",
    "# Define parameters\n",
    "eps_min, eps_mid, eps_rad = sp.symbols('eps_min eps_mid eps_rad', real=True)\n",
    "\n",
    "# Define the objective function f(epsilon)\n",
    "f = 0.5 * (epsilon - eps_min)**2\n",
    "\n",
    "# Define the inequality constraint g(epsilon) <= 0\n",
    "g = (epsilon - eps_mid)**2 - eps_rad**2  # Should be ‚â§ 0\n",
    "\n",
    "# Define the Fischer-Burmeister function\n",
    "def phi_FB(a, b):\n",
    "    return sp.sqrt(a**2 + b**2) - (a + b)\n",
    "\n",
    "# Compute derivatives\n",
    "f_eps = sp.diff(f, epsilon)\n",
    "g_eps = sp.diff(g, epsilon)\n",
    "\n",
    "# Define residuals\n",
    "R1 = f_eps + lambda_ * g_eps                # Stationarity condition\n",
    "R2 = phi_FB(lambda_, -g)                    # Complementarity condition\n",
    "\n",
    "R = sp.Matrix([R1, R2])                     # Residual vector\n",
    "\n",
    "# Variables vector for Jacobian computation\n",
    "A = sp.Matrix([epsilon, lambda_])\n",
    "\n",
    "# Compute the Jacobian matrix\n",
    "J = R.jacobian(A)\n",
    "\n",
    "# Newton-Raphson solver function\n",
    "def newton_raphson(R_func, J_func, epsilon0, lambda0, tol=1e-8, max_iter=100):\n",
    "    x = np.array([epsilon0, lambda0], dtype=float)\n",
    "    for i in range(max_iter):\n",
    "        R_val = np.array(R_func(*x), dtype=float).flatten()\n",
    "        J_val = np.array(J_func(*x), dtype=float)\n",
    "        try:\n",
    "            delta = -np.linalg.solve(J_val, R_val)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print('Singular Jacobian matrix encountered in iteration', i+1)\n",
    "            break\n",
    "        x_new = x + delta\n",
    "        # Enforce lambda >= 0\n",
    "        x_new[1] = max(0, x_new[1])\n",
    "        # Check convergence\n",
    "        if np.linalg.norm(delta) < tol:\n",
    "            print(f'Converged in {i+1} iterations.')\n",
    "            return x_new\n",
    "        x = x_new\n",
    "    print('Did not converge.')\n",
    "    return x\n",
    "\n",
    "# ============================\n",
    "# Case 1: Minimum within range\n",
    "# ============================\n",
    "\n",
    "# Parameters for Case 1\n",
    "params_case1 = {eps_min: 0.0, eps_mid: 0.0, eps_rad: 1.0}\n",
    "\n",
    "# Substitute parameters into symbolic expressions\n",
    "R_subs_case1 = R.subs(params_case1)\n",
    "J_subs_case1 = J.subs(params_case1)\n",
    "\n",
    "# Lambdify the residual and Jacobian functions\n",
    "R_func_case1 = sp.lambdify((epsilon, lambda_), R_subs_case1, modules='numpy')\n",
    "J_func_case1 = sp.lambdify((epsilon, lambda_), J_subs_case1, modules='numpy')\n",
    "\n",
    "# Initial guesses for epsilon and lambda\n",
    "epsilon0_case1 = 0.0\n",
    "lambda0_case1 = 0.0\n",
    "\n",
    "# Solve using Newton-Raphson method\n",
    "print('Solving Case 1...')\n",
    "result_case1 = newton_raphson(R_func_case1, J_func_case1, epsilon0_case1, lambda0_case1)\n",
    "\n",
    "# Output results for Case 1\n",
    "print('\\nCase 1 Result:')\n",
    "print(f'epsilon = {result_case1[0]}')\n",
    "print(f'lambda = {result_case1[1]}')\n",
    "\n",
    "# =============================\n",
    "# Case 2: Minimum outside range\n",
    "# =============================\n",
    "\n",
    "# Parameters for Case 2\n",
    "params_case2 = {eps_min: 2.0, eps_mid: 0.0, eps_rad: 1.0}\n",
    "\n",
    "# Substitute parameters into symbolic expressions\n",
    "R_subs_case2 = R.subs(params_case2)\n",
    "J_subs_case2 = J.subs(params_case2)\n",
    "\n",
    "# Lambdify the residual and Jacobian functions\n",
    "R_func_case2 = sp.lambdify((epsilon, lambda_), R_subs_case2, modules='numpy')\n",
    "J_func_case2 = sp.lambdify((epsilon, lambda_), J_subs_case2, modules='numpy')\n",
    "\n",
    "# Initial guesses for epsilon and lambda\n",
    "epsilon0_case2 = 1.0  # Start at the boundary\n",
    "lambda0_case2 = 1.0   # Positive initial lambda\n",
    "\n",
    "# Solve using Newton-Raphson method\n",
    "print('\\nSolving Case 2...')\n",
    "result_case2 = newton_raphson(R_func_case2, J_func_case2, epsilon0_case2, lambda0_case2)\n",
    "\n",
    "# Output results for Case 2\n",
    "print('\\nCase 2 Result:')\n",
    "print(f'epsilon = {result_case2[0]}')\n",
    "print(f'lambda = {result_case2[1]}')\n",
    "```\n",
    "\n",
    "### Explanation of the Script\n",
    "\n",
    "1. **Symbolic Definitions**:\n",
    "   - We use SymPy to symbolically define the variables, parameters, objective function $ f(\\epsilon) $, and constraint $ g(\\epsilon) $.\n",
    "   - The constraint $ g(\\epsilon) \\leq 0 $ is defined to create an admissible range centered at $ \\epsilon_{\\text{mid}} $ with radius $ \\epsilon_{\\text{rad}} $.\n",
    "   - The Fischer-Burmeister function is used to encode the complementarity condition.\n",
    "\n",
    "2. **Computing Residuals and Jacobian**:\n",
    "   - We compute the residual vector $ R $ consisting of the stationarity condition and the complementarity condition.\n",
    "   - The Jacobian matrix $ J $ of $ R $ with respect to the variables $ \\epsilon $ and $ \\lambda $ is computed symbolically.\n",
    "\n",
    "3. **Newton-Raphson Solver**:\n",
    "   - A Newton-Raphson function `newton_raphson` is defined to iteratively solve $ R(\\epsilon, \\lambda) = 0 $.\n",
    "   - The solver enforces $ \\lambda \\geq 0 $ after each iteration by setting it to zero if it becomes negative.\n",
    "   - Convergence is checked using the norm of the update vector $ \\delta $.\n",
    "\n",
    "4. **Case 1 and Case 2 Setup**:\n",
    "   - For each case, we substitute the respective parameter values into the symbolic expressions and lambdify the residual and Jacobian functions for numerical computation.\n",
    "   - Initial guesses are provided based on the expected behavior of each case.\n",
    "\n",
    "5. **Solving and Output**:\n",
    "   - The Newton-Raphson solver is called for each case, and the results are printed.\n",
    "\n",
    "### Running the Script and Expected Output\n",
    "\n",
    "When you run the script, you should see output similar to the following:\n",
    "\n",
    "```\n",
    "Solving Case 1...\n",
    "Converged in 1 iterations.\n",
    "\n",
    "Case 1 Result:\n",
    "epsilon = 0.0\n",
    "lambda = 0\n",
    "\n",
    "Solving Case 2...\n",
    "Converged in 6 iterations.\n",
    "\n",
    "Case 2 Result:\n",
    "epsilon = 1.0\n",
    "lambda = 1.0\n",
    "```\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **Case 1**:\n",
    "  - The unconstrained minimum at $ \\epsilon_{\\text{min}} = 0 $ is within the admissible range $[-1, 1]$.\n",
    "  - The solver converges quickly, and $ \\lambda $ is zero, indicating the constraint is inactive.\n",
    "  - The solution is $ \\epsilon = 0 $, which is the unconstrained minimum.\n",
    "\n",
    "- **Case 2**:\n",
    "  - The unconstrained minimum at $ \\epsilon_{\\text{min}} = 2 $ is outside the admissible range $[-1, 1]$.\n",
    "  - The solver converges to $ \\epsilon = 1 $, the boundary of the admissible range, with a positive $ \\lambda $, indicating the constraint is active.\n",
    "  - The solution is $ \\epsilon = 1 $ and $ \\lambda = 1 $, reflecting that the minimum occurs at the boundary due to the constraint.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Handling Non-Negativity of Lambda**:\n",
    "  - The Newton-Raphson solver enforces $ \\lambda \\geq 0 $ after each iteration. This reflects the dual feasibility condition of the KKT conditions.\n",
    "\n",
    "- **Convergence**:\n",
    "  - In Case 2, convergence may occur over multiple iterations due to the active constraint and the need for both $ \\epsilon $ and $ \\lambda $ to adjust.\n",
    "\n",
    "- **Customization**:\n",
    "  - You can adjust the parameters $ \\epsilon_{\\text{min}} $, $ \\epsilon_{\\text{mid}} $, and $ \\epsilon_{\\text{rad}} $ to explore different scenarios.\n",
    "  - The initial guesses can also be adjusted for experimentation.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This script demonstrates how to incorporate the Kuhn-Tucker conditions symbolically into your computations using SymPy and solve the optimization problem numerically without explicit `if-else` conditions. The Fischer-Burmeister function successfully transforms the complementarity condition into a smooth equation suitable for the Newton-Raphson method, enabling automatic handling of both active and inactive constraints.\n",
    "\n",
    "Please make sure you have SymPy and NumPy installed in your Python environment to run this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "# Define symbols\n",
    "epsilon, lambda_ = sp.symbols(r'\\varepsilon lambda', real=True, nonnegative=True)\n",
    "\n",
    "# Define parameters\n",
    "eps_min, eps_mid, eps_rad = sp.symbols(r'\\varepsilon_min \\varepsilon_mid \\varepsilon_rad', real=True)\n",
    "\n",
    "mu = sp.symbols(r'\\mu', real=True, positive=True)\n",
    "\n",
    "# Define the objective function f(epsilon)\n",
    "f = 0.5 * (epsilon - eps_min)**2\n",
    "\n",
    "# Define the inequality constraint g(epsilon) <= 0\n",
    "g = (epsilon - eps_mid)**2 - eps_rad**2  # Should be ‚â§ 0\n",
    "\n",
    "# Define the Fischer-Burmeister function\n",
    "def phi_FB(a, b, c):\n",
    "    return sp.sqrt(a**2 + b**2 + c**2) - (a + b)\n",
    "\n",
    "# Compute derivatives\n",
    "f_eps = sp.diff(f, epsilon)\n",
    "g_eps = sp.diff(g, epsilon)\n",
    "\n",
    "A = sp.Matrix([epsilon, lambda_])\n",
    "\n",
    "# # Variables vector for Jacobian computation\n",
    "L = f + lambda_ * g\n",
    "R_ = L.diff(A)\n",
    "\n",
    "R_FB = sp.Matrix.vstack(R_[:-1, :], sp.Matrix([phi_FB(lambda_, -R_[-1, 0], mu)]))\n",
    "J_FB = R_FB.jacobian(A)\n",
    "\n",
    "R_, R_FB, J_FB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_FB_mu = R_FB.subs(mu, 0)\n",
    "J_FB_mu = J_FB.subs(mu, 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get_R = sp.lambdify((A, eps_min, eps_mid, eps_rad), R_FB_mu, 'numpy')\n",
    "get_J = sp.lambdify((A, eps_min, eps_mid, eps_rad), J_FB_mu, 'numpy')\n",
    "get_R_J = sp.lambdify((A, eps_min, eps_mid, eps_rad), (R_FB_mu, J_FB_mu), 'numpy', cse=True)\n",
    "get_g_R_J = sp.lambdify((A, eps_min, eps_mid, eps_rad), (g, R_FB_mu, J_FB_mu), 'numpy', cse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Newton-Raphson solver function\n",
    "tol=1e-8\n",
    "max_iter=10\n",
    "def newton_raphson(eps_0, lam_0, *args):\n",
    "    x = np.array([eps_0, lam_0], dtype=float)\n",
    "    for i in range(max_iter):\n",
    "        g_val, R_val, J_val = get_g_R_J(x, *args)\n",
    "        try:\n",
    "            x -= np.linalg.solve(J_val, R_val[:, 0])\n",
    "        except np.linalg.LinAlgError:\n",
    "            print('Singular Jacobian matrix encountered in iteration', i+1)\n",
    "            break\n",
    "        print(f'Iteration {i+1}: epsilon = {x[0]}, lambda = {x[1]}, g = {g_val}, R_val = {R_val[:,0]}')\n",
    "        x[1] = max(0, x[1])\n",
    "        if np.linalg.norm(R_val) <= tol:\n",
    "            print(f'Converged in {i+1} iterations.')\n",
    "            return x\n",
    "    print('Did not converge.')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Newton-Raphson solver function\n",
    "tol=1e-8\n",
    "max_iter=10\n",
    "def newton_raphson(eps_0, lam_0, *args):\n",
    "    x = np.array([eps_0, lam_0], dtype=float)\n",
    "    for i in range(max_iter):\n",
    "        g_val, R_val, J_val = get_g_R_J(x, *args)\n",
    "        if np.linalg.norm(R_val) <= tol:\n",
    "            print(f'Converged in {i+1} iterations.')\n",
    "            return x\n",
    "        try:\n",
    "            x -= np.linalg.solve(J_val, R_val[:, 0])\n",
    "        except np.linalg.LinAlgError:\n",
    "            print('Singular Jacobian matrix encountered in iteration', i+1)\n",
    "            break\n",
    "        print(f'Iteration {i+1}: epsilon = {x[0]}, lambda = {x[1]}, g = {g_val}, R_val = {R_val[:,0]}')\n",
    "        x[1] = max(0, x[1])\n",
    "    print('Did not converge.')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for Case 1\n",
    "eps_min_c1, eps_mid_c1, eps_rad_c1 = 2, 2, 1\n",
    "result_case1 = newton_raphson(0, 0, eps_min_c1, eps_mid_c1, eps_rad_c1) # eps_0, lam_0, eps_min, eps_mid, eps_rad\n",
    "\n",
    "# Output results for Case 1\n",
    "print('\\nCase 1 Result:')\n",
    "print(f'epsilon = {result_case1[0]}')\n",
    "print(f'lambda = {result_case1[1]}')\n",
    "\n",
    "# =============================\n",
    "# Case 2: Minimum outside range\n",
    "# =============================\n",
    "\n",
    "eps_min_c2, eps_mid_c2, eps_rad_c2 = 2, 0, 1\n",
    "result_case2 = newton_raphson(0, 0, eps_min_c2, eps_mid_c2, eps_rad_c2) # eps_0, lam_0, eps_min, eps_mid, eps_rad\n",
    "\n",
    "# Output results for Case 2\n",
    "print('\\nCase 2 Result:')\n",
    "print(f'epsilon = {result_case2[0]}')\n",
    "print(f'lambda = {result_case2[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "eps_sol1 = result_case1[0]\n",
    "eps_sol2 = result_case2[0]\n",
    "\n",
    "# Prepare the epsilon range for plotting\n",
    "eps1 = np.linspace(eps_sol1-eps_rad_c1*2, eps_sol1+eps_rad_c1*2, 200)\n",
    "eps2 = np.linspace(eps_sol2-eps_rad_c2*2, eps_sol2+eps_rad_c2*2, 200)\n",
    "\n",
    "f_sol1 = 0.5*(eps_sol1 - eps_min_c1)**2\n",
    "# Compute f for Case 1\n",
    "f_case1 = 0.5*(eps1 - eps_min_c1)**2\n",
    "\n",
    "# Case 2 solution\n",
    "f_sol2 = 0.5*(eps_sol2 - eps_min_c2)**2\n",
    "\n",
    "# Compute f for Case 2\n",
    "f_case2 = 0.5*(eps2 - eps_min_c2)**2\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Case 1 plot\n",
    "ax1.plot(eps1, f_case1, label='f(epsilon)')\n",
    "ax1.axvspan(eps_mid_c1-eps_rad_c1, eps_mid_c1+eps_rad_c1, color='green', alpha=0.2, label='Admissible range')\n",
    "ax1.scatter(eps_sol1, f_sol1, color='red', zorder=3, label='Solution')\n",
    "ax1.set_title('Case 1')\n",
    "ax1.set_xlabel('epsilon')\n",
    "ax1.set_ylabel('f(epsilon)')\n",
    "ax1.legend()\n",
    "\n",
    "# Case 2 plot\n",
    "ax2.plot(eps2, f_case2, label='f(epsilon)')\n",
    "ax2.axvspan(eps_mid_c2-eps_rad_c2, eps_mid_c2+eps_rad_c2, color='green', alpha=0.2, label='Admissible range')\n",
    "ax2.scatter(eps_sol2, f_sol2, color='red', zorder=3, label='Solution')\n",
    "ax2.set_title('Case 2')\n",
    "ax2.set_xlabel('epsilon')\n",
    "ax2.set_ylabel('f(epsilon)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = sp.symbols('a b')\n",
    "phi_ab = phi_FB(a, b, mu)\n",
    "phi_ab.diff(sp.Matrix([a, b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# A NumPy version of phi_FB for array inputs\n",
    "def phi_fb_numpy(a, b):\n",
    "    return np.sqrt(a**2 + b**2) - (a + b)\n",
    "\n",
    "# A NumPy version of phi_FB for array inputs\n",
    "def phi_fb_numpy(a, b):\n",
    "    return np.sqrt(a**2 + b**2) - (a + b)\n",
    "\n",
    "a_vals = np.linspace(-3, 3, 60)\n",
    "b_vals = np.linspace(-3, 3, 60)\n",
    "A, B = np.meshgrid(a_vals, b_vals)\n",
    "Z = phi_fb_numpy(A, -B)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "surf = ax.plot_surface(A, B, Z, cmap='viridis', edgecolor='none', alpha=0.2)\n",
    "\n",
    "# Add a white contour line at phi_FB = 0\n",
    "ax.contour(A, B, Z, levels=[0], colors='cadetblue', linewidths=3)\n",
    "ax.contour(A, B, Z, levels=[-1, -1/2, 1, 2, 3, 4, 5, 6], colors='black', linewidths=1)\n",
    "\n",
    "ax.set_xlabel(r'$\\lambda$')\n",
    "ax.set_ylabel(r'$f$')\n",
    "ax.set_zlabel(r'$\\varphi_\\mathrm{FB}(\\lambda,-f)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue you're encountering is rooted in the nature of the Fischer-Burmeister (FB) function and its behavior at the point $(0, 0)$. When both $\\lambda = 0$ and $g(\\epsilon) = 0$, the FB function becomes non-differentiable, leading to a singular Jacobian matrix in the Newton-Raphson method. Let's delve into the details and discuss ways to handle this situation.\n",
    "\n",
    "### **Diagnosis of the Problem**\n",
    "\n",
    "**Non-Differentiability at $(0, 0)$:**\n",
    "\n",
    "The Fischer-Burmeister function is defined as:\n",
    "\n",
    "$$\n",
    "\\phi_{\\text{FB}}(a, b) = \\sqrt{a^2 + b^2} - (a + b)\n",
    "$$\n",
    "\n",
    "At the point $(a, b) = (0, 0)$, the function is continuous and equals zero:\n",
    "\n",
    "$$\n",
    "\\phi_{\\text{FB}}(0, 0) = 0\n",
    "$$\n",
    "\n",
    "However, its partial derivatives involve terms like $\\frac{a}{\\sqrt{a^2 + b^2}}$, which are undefined at $(0, 0)$ due to division by zero. This makes the Jacobian matrix singular when both $\\lambda$ and $g(\\epsilon)$ are zero.\n",
    "\n",
    "**Impact on Newton-Raphson Method:**\n",
    "\n",
    "The Newton-Raphson method relies on the Jacobian matrix to update the variables. A singular Jacobian means that the method cannot compute the direction for the next iteration, leading to failure in convergence.\n",
    "\n",
    "### **Solutions to Handle the Issue**\n",
    "\n",
    "#### **1. Regularize the Fischer-Burmeister Function**\n",
    "\n",
    "To circumvent the non-differentiability at $(0, 0)$, you can introduce a small positive regularization parameter $\\mu > 0$ to smooth the FB function:\n",
    "\n",
    "$$\n",
    "\\phi_{\\text{FB}}^\\mu(a, b) = \\sqrt{a^2 + b^2 + \\mu^2} - (a + b)\n",
    "$$\n",
    "\n",
    "This modification ensures that the denominator in the derivatives is never zero, as $\\sqrt{a^2 + b^2 + \\mu^2} \\geq \\mu > 0$.\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "```python\n",
    "def phi_FB_mu(a, b, mu):\n",
    "    return sp.sqrt(a**2 + b**2 + mu**2) - (a + b)\n",
    "```\n",
    "\n",
    "**Derivatives:**\n",
    "\n",
    "Compute derivatives of the regularized function for use in the Jacobian.\n",
    "\n",
    "#### **2. Use a Semismooth Newton Method**\n",
    "\n",
    "The FB function is semismooth, and specialized methods like the semismooth Newton method can handle nonsmoothness at $(0, 0)$.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Can handle points where the function is not differentiable.\n",
    "- More robust convergence properties for complementarity problems.\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "Modify your solver to accommodate the semismooth Newton method, which involves generalized Jacobians or elements from the B-subdifferential.\n",
    "\n",
    "#### **3. Modify the Initial Guess**\n",
    "\n",
    "Avoid starting exactly at the problematic point by slightly perturbing the initial values:\n",
    "\n",
    "- Set $\\lambda_0$ to a small positive value, e.g., $\\lambda_0 = 1 \\times 10^{-6}$.\n",
    "- If $g(\\epsilon_0) = 0$, adjust $\\epsilon_0$ slightly to ensure $g(\\epsilon_0) \\neq 0$.\n",
    "\n",
    "This can prevent the Jacobian from being singular in the first iteration.\n",
    "\n",
    "#### **4. Implement Safeguards in the Solver**\n",
    "\n",
    "Include checks in your Newton-Raphson implementation to detect singular Jacobians and handle them appropriately:\n",
    "\n",
    "- **Damping Strategies:** Use a line search or damping factor to ensure that updates remain within a safe region.\n",
    "- **Regularization:** Add a small multiple of the identity matrix to the Jacobian when singularity is detected.\n",
    "\n",
    "#### **5. Use Alternative Complementarity Functions**\n",
    "\n",
    "Other complementarity functions may have better properties at $(0, 0)$:\n",
    "\n",
    "- **Chen-Mangasarian Function:**\n",
    "\n",
    "  $$\n",
    "  \\phi_{\\text{CM}}(a, b) = a + b - \\sqrt{(a - b)^2 + \\mu^2}\n",
    "  $$\n",
    "\n",
    "- **Smoothing Approximation:**\n",
    "\n",
    "  $$\n",
    "  \\phi_{\\text{smooth}}(a, b) = \\frac{a + b - \\sqrt{(a - b)^2 + \\mu^2}}{2}\n",
    "  $$\n",
    "\n",
    "These functions can be smoother at the origin and provide better numerical behavior.\n",
    "\n",
    "### **Implementing the Regularized Fischer-Burmeister Function**\n",
    "\n",
    "Here's how you can modify your code to use the regularized FB function:\n",
    "\n",
    "```python\n",
    "def phi_FB_mu(a, b, mu):\n",
    "    return sp.sqrt(a**2 + b**2 + mu**2) - (a + b)\n",
    "\n",
    "# Update residuals with regularization parameter mu\n",
    "mu_value = 1e-6  # Small positive value\n",
    "R2 = phi_FB_mu(lambda_, -g, mu_value)  # Regularized complementarity condition\n",
    "```\n",
    "\n",
    "Compute the derivatives accordingly and update your `lambdify` calls to include `mu_value` as an argument.\n",
    "\n",
    "### **Example Modification in Your Code**\n",
    "\n",
    "```python\n",
    "def get_R_J(A_vals, eps_min_val, eps_mid_val, eps_rad_val, mu_val):\n",
    "    epsilon_val, lambda_val = A_vals\n",
    "    # Compute g(epsilon)\n",
    "    g_val = (epsilon_val - eps_mid_val)**2 - eps_rad_val**2\n",
    "    # Compute f_eps and g_eps\n",
    "    f_eps_val = epsilon_val - eps_min_val\n",
    "    g_eps_val = 2 * (epsilon_val - eps_mid_val)\n",
    "    # Compute R1\n",
    "    R1_val = f_eps_val + lambda_val * g_eps_val\n",
    "    # Compute R2 with regularized FB function\n",
    "    phi_val = np.sqrt(lambda_val**2 + g_val**2 + mu_val**2) - (lambda_val - g_val)\n",
    "    R2_val = phi_val\n",
    "    R_val = np.array([R1_val, R2_val])\n",
    "    # Compute Jacobian components\n",
    "    # ... (compute derivatives carefully to avoid division by zero)\n",
    "    # Return R and J\n",
    "    return R_val, J_val\n",
    "```\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "The singularity you're encountering is a known issue when using the Fischer-Burmeister function for complementarity problems at the origin. By regularizing the FB function, adjusting initial guesses, or employing methods suited for nonsmooth problems, you can resolve the singularity and achieve convergence in your Newton-Raphson solver.\n",
    "\n",
    "### **Recommendations**\n",
    "\n",
    "- **Start with Regularization:** Implement the regularized FB function with a small $\\mu$ and observe if the solver converges.\n",
    "\n",
    "- **Adjust Initial Values:** Slightly perturb your initial guesses to avoid starting exactly at $(0, 0)$.\n",
    "\n",
    "- **Monitor Jacobian Condition Number:** Include diagnostic messages to alert you when the Jacobian is becoming ill-conditioned.\n",
    "\n",
    "- **Consider Specialized Methods:** If regularization doesn't suffice, explore semismooth Newton methods or other algorithms designed for complementarity problems.\n",
    "\n",
    "By addressing the non-differentiability at the core of the issue, you should be able to enhance the robustness of your optimization solver.m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmcs_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
